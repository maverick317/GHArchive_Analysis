{"cells":[{"cell_type":"markdown","source":["#### Imports and Data extracts from GitHub-Archive"],"metadata":{}},{"cell_type":"code","source":["import requests\nimport gzip\nimport json\nimport traceback\n\n\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# events_list is a list of dicts to store all the json files from GitHub-Archive\nevents_list = []\n\n\ndef extract_data():\n  # extracts json.gz from GHArchive and loads events_list with the data\n  \n  for hour in range(12):\n    url = 'http://data.gharchive.org/2015-01-01-'+str(hour)+'.json.gz'\n    r = requests.get(url)\n    file_path = \"./\"+url.split(\"/\")[-1]\n\n    with open(file_path, 'wb') as f:\n        f.write(r.content)\n\n    with gzip.open(file_path, 'rb') as f:\n        for line in f:\n            event = json.loads(line.decode(\"utf-8\"))\n            events_list.append(event)\n  return\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["extract_data()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["###### master_df : Master Dataframe with the raw data from GitHub extract."],"metadata":{}},{"cell_type":"code","source":["# To create dataframe directly from events_list, we need to create schema explicitly.\n# The events_list is nested with more than 7 layers deep and hundreds of fields.\n# Instead dumping it as a json in DBFS (DataBricks File System) and creating \n# dataframe from json automatically infers the schema. \n\n# dumping archive extract into DBFS\nfile_path = \"/dbfs/mnt/2015-01-01.json\"   \nwith open(file_path, 'w+') as fout:\n  json.dump(events_list, fout)\n\n# creating dataframe from json files \nfile_path = \"dbfs:/mnt/2015-01-01.json\"\nmaster_df = spark.read.format(\"json\").load(file_path)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# master_df.printSchema()\n# master_df.count()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["#### Table 1: main_df\n###### main_df is created from master_df with these columns:\n\n* actor_id\n* org_id\n* event_id\n* created_at\n* repo_id\n* event_type"],"metadata":{}},{"cell_type":"code","source":["main_df = master_df.select(F.col(\"actor.id\").alias(\"actor_id\"), \n                           F.col(\"org.id\").alias(\"org_id\"), \n                           F.col(\"id\").alias(\"event_id\"), \n                           F.col(\"created_at\"), \n                           F.col(\"repo.id\").alias(\"repo_id\"), \n                           F.col(\"type\").alias(\"event_type\"))\n# main_df.show()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["##### Data wrangling & exploration.\n* Changed the datatypes of a few columns\n* split the timestamp into hour column. \n* Imputed missing values with appropriate values for string and integer fields"],"metadata":{}},{"cell_type":"code","source":["main_df = main_df.withColumn(\"created_time\", F.split(F.col(\"created_at\"), \"T\").getItem(1))\n\nmain_df = main_df.withColumn(\"created_hour\", F.split(F.col(\"created_time\"), \":\").getItem(0))\\\n                .drop(\"created_at\")\\\n                .drop(\"created_time\")\n\nmain_df = main_df.withColumn(\"actor_id\", main_df[\"actor_id\"].cast(T.LongType()))\\\n                 .withColumn(\"org_id\", main_df[\"org_id\"].cast(T.LongType()))\\\n                 .withColumn(\"event_id\", main_df[\"event_id\"].cast(T.LongType()))\\\n                 .withColumn(\"repo_id\", main_df[\"repo_id\"].cast(T.LongType()))\\\n                 .withColumn(\"created_hour\", main_df[\"created_hour\"].cast(T.IntegerType()))\n        \nmain_df = main_df.fillna({'actor_id': -1, \n                          'org_id': -1, \n                          'event_id': -1, \n                          'repo_id': -1, \n                          'event_type': \"Missing\", \n                          'created_hour': -1})\n\n# print(main_df.show())\n# print(main_df.printSchema())\n# print(main_df.describe().show())\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["##### Data Aggregation on main_df"],"metadata":{}},{"cell_type":"code","source":["print(\"Unique Actors: \", main_df.select('actor_id').distinct().count())\nprint(\"Unique Orgs: \", main_df.select('org_id').distinct().count())\nprint(\"Unique Repos: \", main_df.select('repo_id').distinct().count())\nprint(\"Unique Event Types: \", main_df.select('event_type').distinct().count())\n\nevent_types = [i for i in main_df.select('event_type').distinct().collect()]\nprint(event_types)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["main_df.groupby(\"event_type\")\\\n       .agg(F.count(\"event_id\"))\\\n       .show()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# percentage of org_users out of total user base\n\nprint((main_df.filter(main_df.org_id != -1).select('actor_id').distinct().count()/main_df.select('actor_id').distinct().count())*100)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#percent of repos owned by organiazations out of total repos.\n\nprint((main_df.filter(main_df.org_id != -1).select('repo_id').distinct().count()/main_df.select('repo_id').distinct().count())*100)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["#### Table 2: org_df\n####### org_df has the columns:\n* id\n* name\n* url"],"metadata":{}},{"cell_type":"code","source":["org_df = master_df.select(F.col(\"org.id\").alias(\"org_id\"), \n                          F.col(\"org.login\").alias(\"name\"), \n                          F.col(\"org.url\").alias(\"org_url\"))\\\n                  .dropDuplicates([\"org_id\"])\n\norg_df = org_df.fillna({'org_id': -1, \n                        'name': \"Missing\", \n                        'org_url': \"Missing\"})\n\n# org_df.printSchema()\n# org_df.describe().show()\n# org_df.sample(fraction = 0.01).show()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["#### payload_df\n###### payload_df is the most important table that has data about repos, event payloads with 19 fields and hundreds of nested fields"],"metadata":{}},{"cell_type":"code","source":["# extract all fields and nested fields of payload_df to explore fields of interest\npayload_df = master_df.select(F.col(\"id\").alias(\"event_id\"), \n                              F.col(\"payload.*\"))\n\n# payload_df.printSchema()\n# payload_df.show()\n# payload_df.describe().show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["#### Table 3: repo_df\n###### From payload_df, repository details are of interest to us. \n###### So repo_df with below columns is created\n* event_id\n* repo_name\n* repo_id\n* watchers_count\n* forks_count\n* language_count\n* pull_req_id"],"metadata":{}},{"cell_type":"code","source":["repo_df = payload_df.select(F.col(\"event_id\").alias(\"event_id\"),\n                            F.col(\"pull_request.base.repo.name\").alias(\"repo_name\"),\n                            F.col(\"pull_request.base.repo.owner.organizations_url\").alias(\"org_url\"),\n                            F.col(\"pull_request.base.repo.id\").alias(\"repo_id\"),\n                            F.col(\"pull_request.base.repo.watchers_count\"),\n                            F.col(\"pull_request.base.repo.forks_count\"),\n                            F.col(\"pull_request.base.repo.language\"),\n                            F.col(\"pull_request.id\").alias(\"pull_req_id\")\n                            ).dropna()\n\nrepo_df = repo_df.withColumn(\"event_id\", repo_df[\"event_id\"].cast(T.LongType()))\n\n# repo_df.show()\n# repo_df.printSchema()\n# repo_df.describe().show()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["repo_df.describe().show()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["##### Aggregating and exploring the repo_df"],"metadata":{}},{"cell_type":"code","source":["print(\"Unique Pull Reqs: \", repo_df.select(F.col(\"pull_req_id\")).distinct().count())\nprint(\"Unique Repos: \", repo_df.select(F.col(\"repo_id\")).distinct().count())\nprint(\"Unique Events: \", repo_df.select(F.col(\"event_id\")).distinct().count())\nprint(\"Unique Languages: \", repo_df.select(F.col(\"language\")).distinct().count())\n\nlanguages = [ i for i in repo_df.select(F.col(\"language\")).distinct().collect()]\n# print(languages)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["###### Creating top500_repos_df from repo_df, main_df, org_df"],"metadata":{}},{"cell_type":"code","source":["repo_org_df = main_df.select(F.col(\"repo_id\"), F.col(\"org_id\")).dropDuplicates()\n\nrepo_org_df.createOrReplaceTempView(\"repo_org\")\nrepo_df.createOrReplaceTempView(\"repo\")\n   \ntemp1 = spark.sql(\"select o.org_id, r.* from repo r INNER JOIN repo_org o ON r.repo_id == o.repo_id\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["temp1.createOrReplaceTempView(\"temp1\")\norg_df.createOrReplaceTempView(\"org\")\n\ntemp2 = spark.sql(\"select o.name, t.* from temp1 t INNER JOIN org o ON t.org_id == o.org_id\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["top500_repos_df = temp2.where(F.col(\"name\")!= \"Missing\")\\\n                       .groupby(\"repo_id\")\\\n                       .agg({'watchers_count':'sum', 'forks_count':'sum'})\\\n                       .withColumnRenamed('sum(watchers_count)','watchers_count')\\\n                       .withColumnRenamed('sum(forks_count)', 'forks_count')\\\n                       .orderBy('forks_count', ascending = False)\\\n                       .orderBy('watchers_count', ascending = False)\\\n                       .limit(500)\n              \n#top500_repos_df.show()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["###### Creating top10_orgs_df from top500_repos_df, org_df and aggregating it by organization"],"metadata":{}},{"cell_type":"code","source":["top500_repos_df.createOrReplaceTempView(\"top500_repos_df\")\ntemp2.createOrReplaceTempView(\"temp2\")\n\nrepo_owner_df = spark.sql(\"select t2.name, t1.* from top500_repos_df t1 INNER JOIN temp2 t2 ON t1.repo_id == t2.repo_id\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["top10_orgs_df = repo_owner_df.groupby(\"name\")\\\n                             .agg({'repo_id':'count'})\\\n                             .withColumnRenamed('count(repo_id)','popular_repo_count')\\\n                             .orderBy('popular_repo_count', ascending = False)\\\n                             .limit(10)\n\n# top10_orgs_df.show()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["###### Dumping the DataFrames as csv files to create visualizations in Tableau."],"metadata":{}},{"cell_type":"code","source":["main_df.write.csv('/dbfs/mnt/main.csv')\nrepo_df.write.csv('/dbfs/mnt/repo.csv')\norg_df.write.csv('/dbfs/mnt/org.csv')\ntop500_repos_df.write.csv('/dbfs/mnt/top500_repos.csv')\ntop10_orgs_df.write.csv('/dbfs/mnt/top10_orgs.csv')"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["##### A few event related questions can be answered from the main_df table \n\n\n* ###### There are close to 20+ types of GitHub events possible (as per the GitHub archive data). In the 12 hour data analyzed, there are 14 distinct events. \n* ###### The Data summary on repo, user, organization and event level can also be captured.\n* ###### What are the top events by volume? \n* ###### Does the top 5 events change across org_users and individual_users?"],"metadata":{}},{"cell_type":"markdown","source":["##### Insights about Active users - individual and org_users - can be answered from main_df and org_df table. \n\n* ###### There are two types of user segments in GitHub-Archive data (The archive doesn't contain any details about GitHub Enterprise). \n* ###### Org_users are a part of an organization which pays for collaborating with their team through GitHub. Individual users are other type of users who do not pay for the GitHub service. \n* ###### How different is the event activity type and volume amongst org_users and individual_users? \n* ###### Do org_users trigger more events or individual users?"],"metadata":{}},{"cell_type":"markdown","source":["##### Information about popular repositories, languages and repo owners can be found from repo_df.\n\n* ###### What are the most popular repositories in GitHub? \n* ###### How do we decide the popular repositories? \n* ###### Which organizations own the most popular repositories?\n* ###### Which languages are popular amongst the developers?"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":38}],"metadata":{"name":"GitHub-archive-analysis","notebookId":4029753119174749},"nbformat":4,"nbformat_minor":0}
